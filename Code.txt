Cell 1:
import os, gc, warnings, math, random
warnings.filterwarnings("ignore")

Cell 2:
# --- Imports
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import torch
import torch.nn as nn
import torch.nn.functional as F

print("torch:", torch.__version__)

Cell 3:
# --- Helpers (label inference, binary mapping, numeric features, k-NN graph)
def infer_label_column(df: pd.DataFrame):
    candidates = ["Label","label","Attack","AttackType","attack","Target","Class","class","Outcome"]
    for c in candidates:
        if c in df.columns:
            return c
    return df.columns[-1]

def make_binary_label(series: pd.Series):
    benign_aliases = {"BENIGN","Benign","benign","Normal","normal","Non-Attack","non-attack","Non Attack"}
    y = []
    for v in series.astype(str).fillna("Unknown"):
        if v in benign_aliases or "benign" in v.lower() or "normal" in v.lower():
            y.append(0)
        else:
            y.append(1)
    return np.array(y, dtype=np.int64)

def select_numeric_features(df: pd.DataFrame):
    return df.select_dtypes(include=[np.number]).copy()

def build_knn_graph(X, k=8):
    """
    Build an undirected k-NN graph using cosine metric.
    Returns edge_index (2 x E) and edge_weight (E).
    """
    nbrs = NearestNeighbors(n_neighbors=k+1, metric="cosine").fit(X)
    distances, indices = nbrs.kneighbors(X)
    rows, cols, vals = [], [], []
    n = X.shape[0]
    for i in range(n):
        for j_idx, d in zip(indices[i][1:], distances[i][1:]):
            j = int(j_idx)
            sim = 1.0 - float(d)
            if sim < 0: sim = 0.0
            rows += [i, j]
            cols += [j, i]
            vals += [sim, sim]
    # self-loops
    for i in range(n):
        rows.append(i); cols.append(i); vals.append(1.0)
    coo = pd.DataFrame({"r": rows, "c": cols, "v": vals}).groupby(["r","c"], as_index=False)["v"].sum()
    idx = torch.tensor(coo[["r","c"]].values.T, dtype=torch.long)
    val = torch.tensor(coo["v"].values, dtype=torch.float32)
    return idx, val


Cell 4:
# --- 1) Data loading & preprocessing
CSV_PATH = "cicddos2019_dataset.csv"   # ensure uploaded to notebook environment
assert os.path.exists(CSV_PATH), f"CSV not found at {CSV_PATH} — upload the file."

# Quick schema check
head_df = pd.read_csv(CSV_PATH, nrows=500)
label_col = infer_label_column(head_df)
print("Inferred label column:", label_col)

Cell 5:
# Sample (adjust TARGET_ROWS for larger runs)
TARGET_ROWS = 10000
frames = []
loaded = 0
for chunk in pd.read_csv(CSV_PATH, chunksize=25000):
    if label_col not in chunk.columns:
        label_col = infer_label_column(chunk)
    take = min(3000, len(chunk))
    frames.append(chunk.sample(n=take, random_state=42))
    loaded += take
    if loaded >= TARGET_ROWS:
        break
df = pd.concat(frames, ignore_index=True)
print("Loaded rows:", len(df))

# Labels & numeric features
y = make_binary_label(df[label_col])
X_num = select_numeric_features(df).fillna(0.0)
print("Numeric features:", X_num.shape[1])
scaler = StandardScaler()
X = scaler.fit_transform(X_num.values).astype(np.float32)

Cell 6:
# --- 2) Train/test split + k-NN graph
idx_all = np.arange(len(X))
idx_train, idx_test = train_test_split(idx_all, test_size=0.2, random_state=42, stratify=y)
print("Train nodes:", len(idx_train), "Test nodes:", len(idx_test))

print("Building k-NN graph (might take a minute)...")
edge_index, edge_weight = build_knn_graph(X, k=6)
print("Non-zero edge entries:", edge_index.size(1))

Cell 7:
# --- 3) Minimal GNNs (GCN, GAT, GraphSAGE) without external libs

import math

# ---------- GCN ----------
class GCNLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim, bias=False)
    def forward(self, x, edge_index, edge_weight):
        # row = dst, col = src  (message: src -> dst)
        row, col = edge_index
        n = x.size(0)
        deg = torch.zeros(n, device=x.device).index_add(0, row, edge_weight)
        deg_inv_sqrt = torch.pow(deg + 1e-12, -0.5)
        norm = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]
        xw = self.lin(x)
        out = torch.zeros_like(xw)
        out.index_add_(0, row, norm.unsqueeze(1) * xw[col])
        return out

class GCN(nn.Module):
    def __init__(self, in_dim, hidden, out_dim, dropout=0.4):
        super().__init__()
        self.g1 = GCNLayer(in_dim, hidden)
        self.g2 = GCNLayer(hidden, out_dim)
        self.dropout = dropout
    def forward(self, x, edge_index, edge_weight):
        h = self.g1(x, edge_index, edge_weight)
        h = F.relu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.g2(h, edge_index, edge_weight)
        return h


# ---------- GAT ----------
class GATLayer(nn.Module):
    """
    Single-head GAT layer (destination softmax) implemented with basic PyTorch ops.
    Aggregation: alpha_ij * W x_j where alpha is softmax over incoming neighbors j of node i.
    """
    def __init__(self, in_dim, out_dim, negative_slope=0.2, dropout=0.4):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim, bias=False)
        self.att = nn.Linear(2*out_dim, 1, bias=False)
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        self.dropout = dropout

    def forward(self, x, edge_index, edge_weight=None):
        row, col = edge_index  # dst, src
        h = self.lin(x)        # (N, out_dim)

        # Compute unnormalized attention e_ij = a^T [h_i || h_j]
        e_inputs = torch.cat([h[row], h[col]], dim=1)  # (E, 2*out_dim)
        e = self.leaky_relu(self.att(e_inputs)).squeeze(1)  # (E,)

        # Stable softmax per destination node (row)
        # subtract max per node
        n = x.size(0)
        minus_inf = torch.full((n,), -1e15, device=x.device)
        max_per_row = minus_inf.index_put_((row,), e, accumulate=False)
        # we need the true max per row: do a segmented max using scatter-like emulation
        max_buf = torch.full((n,), -1e15, device=x.device)
        max_buf.index_put_((row,), e, accumulate=True)
        # The accumulate=True on index_put_ is not a max; emulate max by two passes:
        # (1) initialize with -inf, (2) take maximum via index_add on exp trick
        # Simpler: compute softmax with log-sum-exp trick using exp shifted by gathered max.
        # First, compute max per row properly:
        max_per_row = torch.full((n,), -1e15, device=x.device)
        max_per_row.index_put_((row,), torch.maximum(max_per_row[row], e), accumulate=False)
        # Because index_put_ with accumulate=False keeps last occurrence, not max; so do a safer approach:
        # fallback: approximate by grouping with a small binning. For robustness, we do two-step normalization below.

        # Practical stable softmax without torch_scatter:
        # 1) subtract global max to avoid overflow (good enough in practice)
        e_shift = e - e.max()
        exp_e = torch.exp(e_shift)
        denom = torch.zeros(n, device=x.device).index_add(0, row, exp_e)
        alpha = exp_e / (denom[row] + 1e-12)
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)

        # Aggregate messages
        out = torch.zeros_like(h)
        out.index_add_(0, row, alpha.unsqueeze(1) * h[col])
        return out

class GAT(nn.Module):
    def __init__(self, in_dim, hidden, out_dim, dropout=0.4):
        super().__init__()
        self.g1 = GATLayer(in_dim, hidden, dropout=dropout)
        self.g2 = GATLayer(hidden, out_dim, dropout=dropout)
        self.dropout = dropout
    def forward(self, x, edge_index, edge_weight):
        h = self.g1(x, edge_index, edge_weight)
        h = F.elu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.g2(h, edge_index, edge_weight)
        return h


# ---------- GraphSAGE ----------
class SAGEConv(nn.Module):
    """
    Mean-aggregator GraphSAGE.
    out = W_self * x + W_neigh * mean(neigh_feats)
    """
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin_self  = nn.Linear(in_dim, out_dim, bias=False)
        self.lin_neigh = nn.Linear(in_dim, out_dim, bias=False)

    def forward(self, x, edge_index, edge_weight=None):
        row, col = edge_index  # dst, src
        n = x.size(0)

        # sum neighbor features per destination
        agg = torch.zeros_like(self.lin_neigh(x))
        # compute neighbor feature sum on original features, then linear
        neigh_sum = torch.zeros_like(x)
        neigh_sum.index_add_(0, row, x[col])

        # degree (number of incoming neighbors)
        deg = torch.zeros(n, device=x.device).index_add(0, row, torch.ones_like(row, dtype=torch.float32))
        deg = deg.clamp_min(1.0)
        neigh_mean = neigh_sum / deg.unsqueeze(1)

        out = self.lin_self(x) + self.lin_neigh(neigh_mean)
        return out

class GraphSAGE(nn.Module):
    def __init__(self, in_dim, hidden, out_dim, dropout=0.4):
        super().__init__()
        self.s1 = SAGEConv(in_dim, hidden)
        self.s2 = SAGEConv(hidden, out_dim)
        self.dropout = dropout
    def forward(self, x, edge_index, edge_weight):
        h = self.s1(x, edge_index, edge_weight)
        h = F.relu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.s2(h, edge_index, edge_weight)
        return h

Cell 8:
# --- 4) Federated simulation: split train nodes among clients; local updates; heterogeneous models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

x_t = torch.tensor(X, dtype=torch.float32, device=device)
y_t = torch.tensor(y, dtype=torch.long, device=device)
edge_index = edge_index.to(device)
edge_weight = edge_weight.to(device)

def train_local(model, x, edge_index, edge_weight, y, idx_train, epochs=3, lr=5e-3, wd=1e-4):
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    for _ in range(epochs):
        opt.zero_grad()
        logits = model(x, edge_index, edge_weight)
        loss = F.cross_entropy(logits[idx_train], y[idx_train])
        loss.backward()
        opt.step()
    return model

num_clients = 100
rng = np.random.default_rng(42)
client_indices = np.array_split(rng.permutation(idx_train), num_clients)

in_dim = x_t.shape[1]
hidden = 64
out_dim = 2

# --- assign models: 0-33 GCN (34 clients), 34-66 GAT (33), 67-99 GraphSAGE (33)
def make_model(cid):
    if cid < 34:
        return GCN(in_dim, hidden, out_dim, dropout=0.4).to(device)
    elif cid < 67:
        return GAT(in_dim, hidden, out_dim, dropout=0.4).to(device)
    else:
        return GraphSAGE(in_dim, hidden, out_dim, dropout=0.4).to(device)

local_models = [make_model(cid) for cid in range(num_clients)]

for cid in range(num_clients):
    idx_tr = torch.tensor(client_indices[cid], dtype=torch.long, device=device)
    model_name = local_models[cid].__class__.__name__
    print(f"Training client {cid+1} ({model_name}) on {len(idx_tr)} nodes...")
    train_local(local_models[cid], x_t, edge_index, edge_weight, y_t, idx_tr, epochs=3, lr=5e-3)


Cell 9:
# --- 5) FGNN-E ensemble inference & evaluation (GCN + GAT + GraphSAGE)
idx_test_t = torch.tensor(idx_test, dtype=torch.long, device=device)

with torch.no_grad():
    # stack logits from ALL local models and average (logit-level ensemble is better than prob avg)
    all_logits = []
    for m in local_models:
        all_logits.append(m(x_t, edge_index, edge_weight))
    logits = torch.stack(all_logits, dim=0).mean(dim=0)

    probs = F.softmax(logits[idx_test_t], dim=1)
    preds = probs.argmax(dim=1).cpu().numpy()
    y_true = y_t[idx_test_t].cpu().numpy()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

acc = accuracy_score(y_true, preds)
prec, rec, f1, _ = precision_recall_fscore_support(y_true, preds, average="binary", zero_division=0)

print("\nFGNN-E (GCN + GAT + GraphSAGE) — Test Metrics")
print("Accuracy:", round(acc,4), "Precision:", round(prec,4), "Recall:", round(rec,4), "F1:", round(f1,4))
print("\nClassification Report:\n", classification_report(y_true, preds, target_names=["Benign","DDoS"]))


Cell 10:
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Benign","DDoS"], yticklabels=["Benign","DDoS"])
plt.title("Confusion Matrix - FGNN-E")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


Cell 11:
from sklearn.metrics import roc_curve, auc, precision_recall_curve

fpr, tpr, _ = roc_curve(y_true, probs[:,1].cpu().numpy())
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label="AUC = %.2f" % roc_auc)
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - FGNN-E")
plt.legend()
plt.show()


Cell 12:
import networkx as nx

subset_edges = edge_index[:, :200].cpu().numpy()
G = nx.Graph()
for u,v in zip(subset_edges[0], subset_edges[1]):
    G.add_edge(int(u), int(v))

plt.figure(figsize=(7,7))
pos = nx.spring_layout(G, seed=42, k=0.2)
nx.draw(G, pos, node_size=30, node_color="skyblue", edge_color="gray")
plt.title("Sample Graph of Network Flows")
plt.show()

Cell 13:
from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, _ = precision_recall_curve(y_true, probs[:,1].cpu().numpy())
avg_precision = average_precision_score(y_true, probs[:,1].cpu().numpy())

plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve - FGNN-E")
plt.legend()
plt.show()



